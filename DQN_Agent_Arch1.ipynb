{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, environment, action_size, state_size, hyperparameter):\n",
    "        # Define size of state and action\n",
    "        self.environment = environment\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # hyper parameters for the DQN\n",
    "        self.discount_factor = hyperparameter['discount_factor'] or 0.95\n",
    "        self.learning_rate = hyperparameter['learning_rate'] or 0.01\n",
    "        self.epsilon = hyperparameter['epsilon'] or 1\n",
    "        self.epsilon_max = hyperparameter['epsilon_max'] or 1\n",
    "        self.epsilon_decay = hyperparameter['epsilon_decay'] or 0.0009\n",
    "        self.epsilon_min = hyperparameter['epsilon_min'] or 0.01\n",
    "        \n",
    "        self.batch_size = hyperparameter['batch_size'] or 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    \n",
    "    def set_epsilon(self, episode):\n",
    "        self.epsilon = self.epsilon_min + (\n",
    "            self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay * episode)\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        return self.epsilon\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # hidden layers\n",
    "        model.add(Dense(36, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(36, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        get action in a state according to an epsilon-greedy approach\n",
    "        \"\"\"\n",
    "        (possible_actions_index, actions) = self.environment.requests(state)\n",
    "\n",
    "        # 0.011\n",
    "        if random.uniform(self.epsilon_max, self.epsilon_min) < self.get_epsilon(): \n",
    "            index = random.randrange(len(possible_actions_index))\n",
    "            return (possible_actions_index[index], actions[index])\n",
    "\n",
    "        else:\n",
    "            state = np.array(self.environment.state_encod_arch1(state)).reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            \n",
    "            filter_q_value = np.take(q_value[0], possible_actions_index)\n",
    "            \n",
    "            if q_value[0][0] > np.max(filter_q_value):\n",
    "                return (0, actions[-1])\n",
    "            else:\n",
    "                return (possible_actions_index[np.argmax(filter_q_value)], actions[np.argmax(filter_q_value)])\n",
    "\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        # append the tuple (s, a, r, s', done) to memory (replay buffer) after every action\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        train the neural network on a minibatch. Input to the network is the states,\n",
    "        output is the target q-value corresponding to each action.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            \n",
    "            # sample minibatch from memory\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = minibatch[i]\n",
    "                update_input[i] = self.environment.state_encod_arch1(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = self.environment.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "            \n",
    "            # for testing\n",
    "            for i in range(self.batch_size):\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "#             # update the target values\n",
    "#             for i in range(self.batch_size):\n",
    "#                 if done[i]:\n",
    "#                     target[i][actions[i]] = rewards[i]\n",
    "#                 else: # non-terminal state\n",
    "#                     target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "            # model fit\n",
    "            history = self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "            # print(history.history['loss'])\n",
    "            return history.history\n",
    "\n",
    "    def get_qvalue(self, state):\n",
    "        state = np.array(self.environment.state_encod_arch1(state)).reshape(1, self.state_size)\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def save_model_weights(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 4000\n",
    "\n",
    "max_car_drive_time = 30 * 24\n",
    "\n",
    "Hyperparameters = {\n",
    "    'discount_factor': 0.95,\n",
    "    'learning_rate' : 0.001,\n",
    "    'epsilon': 1,\n",
    "    'epsilon_max' : 1,\n",
    "    'epsilon_decay' : 0.003,\n",
    "    'epsilon_min': 0.01,\n",
    "    'batch_size' : 64\n",
    "}\n",
    "\n",
    "state_size = 36\n",
    "\n",
    "\n",
    "env = CabDriver()\n",
    "(action_space, _, _) = env.reset()\n",
    "\n",
    "action_size = len(action_space)\n",
    "\n",
    "agent = DQNAgent(environment= env, action_size= action_size, state_size= state_size, hyperparameter= Hyperparameters)\n",
    "\n",
    "(rewards_per_episode, nn_loss_per_episode) = ([],[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episdoe:50 DriveTime:722.0 NN_loss:[1598.135009765625] Reward:-361.0 Epsilon:0.8621008966608072\n",
      "Episdoe:100 DriveTime:720.0 NN_loss:[2169.2509765625] Reward:279.0 Epsilon:0.7434100384749007\n",
      "Episdoe:150 DriveTime:720.0 NN_loss:[1369.658447265625] Reward:558.0 Epsilon:0.6412518701055556\n",
      "Episdoe:200 DriveTime:725.0 NN_loss:[1578.6802978515625] Reward:713.0 Epsilon:0.5533235197330862\n",
      "Episdoe:250 DriveTime:721.0 NN_loss:[1207.4180908203125] Reward:670.0 Epsilon:0.47764288721360454\n",
      "Episdoe:300 DriveTime:723.0 NN_loss:[1281.0433349609375] Reward:597.0 Epsilon:0.4125039631431931\n",
      "Episdoe:350 DriveTime:722.0 NN_loss:[1168.4063720703125] Reward:503.0 Epsilon:0.35643837162004377\n",
      "Episdoe:400 DriveTime:724.0 NN_loss:[2019.6619873046875] Reward:781.0 Epsilon:0.3081822697930801\n",
      "Episdoe:450 DriveTime:720.0 NN_loss:[988.4712524414062] Reward:801.0 Epsilon:0.2666478580394326\n",
      "Episdoe:500 DriveTime:726.0 NN_loss:[489.3177185058594] Reward:960.0 Epsilon:0.23089885854694553\n",
      "Episdoe:550 DriveTime:720.0 NN_loss:[616.3397216796875] Reward:855.0 Epsilon:0.20012940953454655\n",
      "Episdoe:600 DriveTime:727.0 NN_loss:[524.7190551757812] Reward:1054.0 Epsilon:0.17364589933937066\n",
      "Episdoe:650 DriveTime:726.0 NN_loss:[827.3421630859375] Reward:1275.0 Epsilon:0.15085133087064845\n",
      "Episdoe:700 DriveTime:725.0 NN_loss:[447.53558349609375] Reward:956.0 Epsilon:0.13123186397045208\n",
      "Episdoe:750 DriveTime:721.0 NN_loss:[752.099365234375] Reward:1318.0 Epsilon:0.11434523231624569\n",
      "Episdoe:800 DriveTime:728.0 NN_loss:[419.20843505859375] Reward:1157.0 Epsilon:0.09981077375651838\n",
      "Episdoe:850 DriveTime:725.0 NN_loss:[1275.4376220703125] Reward:1082.0 Epsilon:0.08730084934114159\n",
      "Episdoe:900 DriveTime:720.0 NN_loss:[393.54254150390625] Reward:1323.0 Epsilon:0.07653345761235225\n",
      "Episdoe:950 DriveTime:726.0 NN_loss:[414.601806640625] Reward:1302.0 Epsilon:0.06726587766609007\n",
      "Episdoe:1000 DriveTime:726.0 NN_loss:[1.762951135635376] Reward:1185.0 Epsilon:0.05928919768418531\n",
      "Episdoe:1050 DriveTime:720.0 NN_loss:[377.6142272949219] Reward:783.0 Epsilon:0.05242360559836979\n",
      "Episdoe:1100 DriveTime:726.0 NN_loss:[421.0963439941406] Reward:1311.0 Epsilon:0.046514335727227595\n",
      "Episdoe:1150 DriveTime:721.0 NN_loss:[1094.746826171875] Reward:1354.0 Epsilon:0.04142818001428726\n",
      "Episdoe:1200 DriveTime:727.0 NN_loss:[622.7744140625] Reward:1369.0 Epsilon:0.03705048522281963\n",
      "Episdoe:1250 DriveTime:724.0 NN_loss:[1034.8939208984375] Reward:1168.0 Epsilon:0.03328256839744902\n",
      "Episdoe:1300 DriveTime:723.0 NN_loss:[420.6986083984375] Reward:1524.0 Epsilon:0.030039492331346347\n",
      "Episdoe:1350 DriveTime:728.0 NN_loss:[211.4349822998047] Reward:1319.0 Epsilon:0.02724815089309858\n",
      "Episdoe:1400 DriveTime:731.0 NN_loss:[179.6636505126953] Reward:1484.0 Epsilon:0.024845621052272927\n",
      "Episdoe:1450 DriveTime:720.0 NN_loss:[539.9065551757812] Reward:1224.0 Epsilon:0.022777744454675064\n",
      "Episdoe:1500 DriveTime:723.0 NN_loss:[1.686002254486084] Reward:1326.0 Epsilon:0.02099790657285988\n",
      "Episdoe:1550 DriveTime:727.0 NN_loss:[626.7864990234375] Reward:1405.0 Epsilon:0.01946598591123807\n",
      "Episdoe:1600 DriveTime:727.0 NN_loss:[1602.096435546875] Reward:1360.0 Epsilon:0.01814744957852983\n",
      "Episdoe:1650 DriveTime:721.0 NN_loss:[1.8940376043319702] Reward:1291.0 Epsilon:0.017012574839761596\n",
      "Episdoe:1700 DriveTime:726.0 NN_loss:[390.76727294921875] Reward:1293.0 Epsilon:0.016035779099860478\n",
      "Episdoe:1750 DriveTime:726.0 NN_loss:[225.93450927734375] Reward:1293.0 Epsilon:0.015195043215189571\n",
      "Episdoe:1800 DriveTime:724.0 NN_loss:[406.9376220703125] Reward:1177.0 Epsilon:0.01447141513318654\n",
      "Episdoe:1850 DriveTime:720.0 NN_loss:[419.04248046875] Reward:1521.0 Epsilon:0.01384858267104137\n",
      "Episdoe:1900 DriveTime:726.0 NN_loss:[438.8162536621094] Reward:1446.0 Epsilon:0.01331250580289656\n",
      "Episdoe:1950 DriveTime:726.0 NN_loss:[236.33961486816406] Reward:1347.0 Epsilon:0.012851100166507357\n",
      "Episdoe:2000 DriveTime:726.0 NN_loss:[920.9922485351562] Reward:1293.0 Epsilon:0.012453964654899695\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for episode in range(1, Episodes + 1):\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    agent_total_time = 0\n",
    "    \n",
    "    (_, _, curr_state) = env.reset()\n",
    "    \n",
    "    while not done:        \n",
    "        (action_index, action) = agent.get_action(curr_state)\n",
    "        \n",
    "        (next_state, reward, total_worked_hours) = env.step(curr_state, action, Time_matrix)\n",
    "        \n",
    "        agent.append_sample(curr_state, action_index, reward, next_state, done)\n",
    "        \n",
    "        history = agent.train_model()\n",
    "\n",
    "        total_reward += reward\n",
    "        curr_state = next_state\n",
    "        \n",
    "        agent_total_time += total_worked_hours\n",
    "        if agent_total_time >= max_car_drive_time:\n",
    "            done = True\n",
    "            \n",
    "    agent.set_epsilon(episode)\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    nn_loss_per_episode.append(history['loss'])\n",
    "    \n",
    "    if episode % 1000 == 0:\n",
    "        agent.save_model_weights(name=\"model_weights.h5\")\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print('Episdoe:{} DriveTime:{} NN_loss:{} Reward:{} Epsilon:{}'.format(\n",
    "            episode, agent_total_time, history['loss'], total_reward, agent.get_epsilon()))\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(0, len(rewards_per_episode)))\n",
    "plt.plot(xaxis,np.asarray(rewards_per_episode))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(0, len(nn_loss_per_episode)))\n",
    "plt.plot(xaxis,np.asarray(nn_loss_per_episode))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(2000, 4000))\n",
    "plt.plot(xaxis,np.asarray(nn_loss_per_episode)[-2000:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,4000)\n",
    "epsilon = []\n",
    "for i in range(0,4000):\n",
    "    epsilon.append(0.01 + (1 - 0.01) * np.exp(-0.003*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
