{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, environment, action_size, state_size, hyperparameter):\n",
    "        # Define size of state and action\n",
    "        self.environment = environment\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # hyper parameters for the DQN\n",
    "        self.discount_factor = hyperparameter['discount_factor'] or 0.95\n",
    "        self.learning_rate = hyperparameter['learning_rate'] or 0.01\n",
    "        self.epsilon = hyperparameter['epsilon'] or 1\n",
    "        self.epsilon_max = hyperparameter['epsilon_max'] or 1\n",
    "        self.epsilon_decay = hyperparameter['epsilon_decay'] or 0.0009\n",
    "        self.epsilon_min = hyperparameter['epsilon_min'] or 0.01\n",
    "        \n",
    "        self.batch_size = hyperparameter['batch_size'] or 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    \n",
    "    def set_epsilon(self, episode):\n",
    "        self.epsilon = self.epsilon_min + (\n",
    "            self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay * episode)\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        return self.epsilon\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # hidden layers\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        get action in a state according to an epsilon-greedy approach\n",
    "        \"\"\"\n",
    "        (possible_actions_index, actions) = self.environment.requests(state)\n",
    "        if np.random.rand() <= self.get_epsilon(): \n",
    "            index = random.randrange(len(possible_actions_index))\n",
    "            return (possible_actions_index[index], actions[index])\n",
    "\n",
    "        else:\n",
    "            state = np.array(self.environment.state_encod_arch1(state)).reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            \n",
    "            filter_q_value = np.take(q_value[0], possible_actions_index)\n",
    "            \n",
    "            if q_value[0][0] > np.max(filter_q_value):\n",
    "                return (0, actions[-1])\n",
    "            else:\n",
    "                return (possible_actions_index[np.argmax(filter_q_value)], actions[np.argmax(filter_q_value)])\n",
    "\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        # append the tuple (s, a, r, s', done) to memory (replay buffer) after every action\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        train the neural network on a minibatch. Input to the network is the states,\n",
    "        output is the target q-value corresponding to each action.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            \n",
    "            # sample minibatch from memory\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = minibatch[i]\n",
    "                update_input[i] = self.environment.state_encod_arch1(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = self.environment.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "            # model fit\n",
    "            history = self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "            # print(history.history['loss'])\n",
    "            return history.history\n",
    "\n",
    "\n",
    "    def save_model_weights(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 100\n",
    "\n",
    "max_car_drive_time = 30 * 24\n",
    "\n",
    "Hyperparameters = {\n",
    "    'discount_factor': 0.95,\n",
    "    'learning_rate' : 0.01,\n",
    "    'epsilon': 1,\n",
    "    'epsilon_max' : 1,\n",
    "    'epsilon_decay' : 0.1,\n",
    "    'epsilon_min': 0.01,\n",
    "    'batch_size' : 32\n",
    "}\n",
    "\n",
    "state_size = 36\n",
    "\n",
    "\n",
    "env = CabDriver()\n",
    "(action_space, _, _) = env.reset()\n",
    "\n",
    "action_size = len(action_space)\n",
    "\n",
    "agent = DQNAgent(environment= env, action_size= action_size, state_size= state_size, hyperparameter= Hyperparameters)\n",
    "\n",
    "(rewards_per_episode, nn_loss_per_episode) = ([],[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 727.0\n",
      "Done 725.0\n",
      "Done 722.0\n",
      "Done 732.0\n",
      "Done 728.0\n",
      "Done 727.0\n",
      "Done 723.0\n",
      "Done 721.0\n",
      "Done 725.0\n",
      "Done 723.0\n",
      "Done 726.0\n",
      "Done 732.0\n",
      "Done 720.0\n",
      "Done 724.0\n",
      "Done 720.0\n",
      "Done 720.0\n",
      "Done 721.0\n",
      "Done 720.0\n",
      "Done 725.0\n",
      "Done 720.0\n",
      "Done 722.0\n",
      "Done 722.0\n",
      "Done 722.0\n",
      "Done 721.0\n",
      "Done 725.0\n",
      "Done 724.0\n",
      "Done 724.0\n",
      "Done 723.0\n",
      "Done 720.0\n",
      "Done 722.0\n",
      "Done 720.0\n",
      "Done 721.0\n",
      "Done 728.0\n",
      "Done 724.0\n",
      "Done 728.0\n",
      "Done 722.0\n",
      "Done 723.0\n",
      "Done 720.0\n",
      "Done 722.0\n",
      "Done 725.0\n",
      "Done 733.0\n",
      "Done 724.0\n",
      "Done 720.0\n",
      "Done 728.0\n",
      "Done 721.0\n",
      "Done 728.0\n",
      "Done 727.0\n",
      "Done 720.0\n",
      "Done 723.0\n",
      "Done 723.0\n",
      "Done 725.0\n",
      "Done 727.0\n",
      "Done 722.0\n",
      "Done 726.0\n",
      "Done 720.0\n",
      "Done 726.0\n",
      "Done 721.0\n",
      "Done 726.0\n",
      "Done 721.0\n",
      "Done 727.0\n",
      "Done 721.0\n",
      "Done 721.0\n",
      "Done 731.0\n",
      "Done 721.0\n",
      "Done 720.0\n",
      "Done 722.0\n",
      "Done 721.0\n",
      "Done 722.0\n",
      "Done 729.0\n",
      "Done 724.0\n",
      "Done 725.0\n",
      "Done 721.0\n",
      "Done 724.0\n",
      "Done 727.0\n",
      "Done 725.0\n",
      "Done 723.0\n",
      "Done 728.0\n",
      "Done 722.0\n",
      "Done 723.0\n",
      "Done 724.0\n",
      "Done 724.0\n",
      "Done 721.0\n",
      "Done 723.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for episode in range(Episodes):\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    agent_total_time = 0\n",
    "    \n",
    "    (_, _, curr_state) = env.reset()\n",
    "    \n",
    "    while not done:        \n",
    "        (action_index, action) = agent.get_action(curr_state)\n",
    "        \n",
    "        (next_state, reward, total_worked_hours) = env.step(curr_state, action, Time_matrix)\n",
    "        \n",
    "        agent.append_sample(curr_state, action_index, reward, next_state, done)\n",
    "        \n",
    "        nn_loss = agent.train_model()\n",
    "        total_reward += reward\n",
    "        \n",
    "        curr_state = next_state\n",
    "        \n",
    "        agent_total_time += total_worked_hours\n",
    "        if agent_total_time >= max_car_drive_time:\n",
    "            done = True\n",
    "            print('Done {}'.format(agent_total_time))\n",
    "        \n",
    "    \n",
    "    rewards_per_episode.append(total_reward)\n",
    "    nn_loss_per_episode.append(nn_loss)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-168.0,\n",
       " 54.0,\n",
       " -45.0,\n",
       " -37.0,\n",
       " 90.0,\n",
       " 335.0,\n",
       " -188.0,\n",
       " 218.0,\n",
       " 156.0,\n",
       " 62.0,\n",
       " 161.0,\n",
       " -60.0,\n",
       " 297.0,\n",
       " -135.0,\n",
       " 22.0,\n",
       " -267.0,\n",
       " 248.0,\n",
       " -237.0,\n",
       " -114.0,\n",
       " -213.0,\n",
       " 23.0,\n",
       " 3.0,\n",
       " -57.0,\n",
       " -70.0,\n",
       " 26.0,\n",
       " 72.0,\n",
       " 79.0,\n",
       " -166.0,\n",
       " -60.0,\n",
       " 12.0,\n",
       " -49.0,\n",
       " 108.0,\n",
       " 191.0,\n",
       " -229.0,\n",
       " -164.0,\n",
       " -113.0,\n",
       " -83.0,\n",
       " -289.0,\n",
       " -36.0,\n",
       " -94.0,\n",
       " 48.0,\n",
       " -109.0,\n",
       " 14.0,\n",
       " 196.0,\n",
       " 79.0,\n",
       " 94.0,\n",
       " 69.0,\n",
       " -14.0,\n",
       " 35.0,\n",
       " 151.0,\n",
       " 166.0,\n",
       " -164.0,\n",
       " 178.0,\n",
       " 270.0,\n",
       " 110.0,\n",
       " -51.0,\n",
       " -100.0,\n",
       " -171.0,\n",
       " -25.0,\n",
       " -230.0,\n",
       " 127.0,\n",
       " -183.0,\n",
       " 178.0,\n",
       " -62.0,\n",
       " 62.0,\n",
       " 76.0,\n",
       " -18.0,\n",
       " 188.0,\n",
       " -86.0,\n",
       " -19.0,\n",
       " -51.0,\n",
       " 13.0,\n",
       " 144.0,\n",
       " 204.0,\n",
       " -108.0,\n",
       " 60.0,\n",
       " 103.0,\n",
       " 283.0,\n",
       " -42.0,\n",
       " 386.0,\n",
       " 9.0,\n",
       " 26.0,\n",
       " 121.0,\n",
       " -87.0,\n",
       " 25.0,\n",
       " 22.0,\n",
       " -258.0,\n",
       " -83.0,\n",
       " -276.0,\n",
       " 0.0,\n",
       " -186.0,\n",
       " 264.0,\n",
       " -36.0,\n",
       " 47.0,\n",
       " -35.0,\n",
       " -41.0,\n",
       " 138.0,\n",
       " 75.0,\n",
       " 228.0,\n",
       " -72.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,100)\n",
    "epsilon = []\n",
    "for i in range(0,100):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.1*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
